{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas pandas_datareader matplotlib yfinance scikit-learn xgboost tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat data analyzing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdata\n",
    "from datetime import datetime as dtim, timedelta as tdel\n",
    "\n",
    "# projecting data libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# financial data libraries\n",
    "import yfinance as yf\n",
    "\n",
    "# ML libraries\n",
    "import mlflow as mlf\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler as ssc\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error as mse,\n",
    "    mean_absolute_error as mae,\n",
    "    r2_score as r2s,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential as seqMD\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout as dpMD\n",
    "\n",
    "# other miscellaneous libraries\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Intake & Refine Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "\n",
    "    # Constructor for pipeline.\n",
    "    def __init__(self, asset, trading_day_window=30, active_total_trading_days=252):\n",
    "        self.asset = asset\n",
    "        self.trading_window = trading_day_window\n",
    "        self.active_days = active_total_trading_days\n",
    "        self.scaling_factor = np.sqrt(active_total_trading_days)\n",
    "        self.dataframe = None\n",
    "        self.features = None\n",
    "        self.scaled_features = None\n",
    "\n",
    "    ## Fetching past for the asset for this interval of time.\n",
    "    def fetch_financial_data(self, start_date, end_date):\n",
    "\n",
    "        # this dataframe holds OHLCV data. (Open, HIgh, Low, Close, Volume).\n",
    "        dataframe = yf.download(tickers=self.asset, start=start_date, end=end_date)\n",
    "\n",
    "        ## Calculating returns.\n",
    "        dataframe[\"Returns\"] = np.log(dataframe[\"Close\"] / dataframe[\"Close\"].shift(1))\n",
    "\n",
    "        ## Returning the dataframe with returns calculated.\n",
    "        self.dataframe = dataframe\n",
    "        return dataframe\n",
    "\n",
    "    ## Calculating Realized Volatility -> Standard Deviation of Returns from the Mean Return.\n",
    "    def calculate_realized_volatility(self):\n",
    "        # taken a month by default can change it accordingly.\n",
    "\n",
    "        ## Calculating the Annual Volatility using the Rolling Window Standard Deviation and Scaling it.\n",
    "        self.dataframe[\"RealizedVolatility\"] = (\n",
    "            self.dataframe[\"Returns\"].rolling(window=self.trading_window).std()\n",
    "        ) * (self.scaling_factor)\n",
    "\n",
    "        ## Calculating High-Low Volatility\n",
    "        self.dataframe[\"HighLowVolatility\"] = np.log(\n",
    "            self.dataframe[\"High\"] / self.dataframe[\"Low\"]\n",
    "        )\n",
    "\n",
    "        ## Calculating GARMAN KLASS Volatility using the mathematical formula\n",
    "        self.dataframe[\"GarmanKlassVolatility\"] = np.sqrt(\n",
    "            (((np.log(self.dataframe[\"High\"] / self.dataframe[\"Low\"])) ** 2) * 0.5)\n",
    "            - (\n",
    "                ((2 * (np.log(2))) - 1)\n",
    "                * ((np.log(self.dataframe[\"Close\"] / self.dataframe[\"Open\"])) ** 2)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self.dataframe\n",
    "\n",
    "    ## Technical Indicators and features for Volatility Prediction\n",
    "    def create_feature_set(self, Moving_AvgStd_window=22, lookback_period=[5, 10, 22]):\n",
    "\n",
    "        # Ensuring the returns are present\n",
    "        if \"Returns\" not in self.dataframe.columns:\n",
    "            self.dataframe[\"Returns\"] = np.log(\n",
    "                self.dataframe[\"Close\"] / self.dataframe[\"Close\"].shift(1)\n",
    "            )\n",
    "\n",
    "        ## Calculating features\n",
    "        if self.features is None:\n",
    "            self.features = pd.DataFrame(index=self.dataframe.index)\n",
    "\n",
    "        ### Volume-based features\n",
    "        #### this is the rolling mean\n",
    "        self.features[\"Volume_MovAvg\"] = (\n",
    "            self.dataframe[\"Volume\"].rolling(window=Moving_AvgStd_window).mean()\n",
    "        )\n",
    "        #### this is the rolling standard deviation\n",
    "        self.features[\"Volume_StdDev\"] = (\n",
    "            self.dataframe[\"Volume\"].rolling(window=Moving_AvgStd_window).std()\n",
    "        )\n",
    "\n",
    "        ### Price-based features\n",
    "        for period in lookback_period:\n",
    "\n",
    "            #### Moving avgs\n",
    "            self.features[f\"Price_MovAvg_{period}\"] = (\n",
    "                self.dataframe[\"Close\"].rolling(window=period).mean()\n",
    "            )\n",
    "            self.features[f\"Price_StdDev_{period}\"] = (\n",
    "                self.dataframe[\"Close\"].rolling(window=period).std()\n",
    "            )\n",
    "\n",
    "            #### RSI -> Relative Strength Index ==> OverBought or OverSold\n",
    "            delta = self.dataframe[\"Close\"].diff()\n",
    "            avg_gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "            ##### if the where condition is not satisfied then we replace it by 0\n",
    "            avg_loss = (delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "            relative_strength = avg_gain / avg_loss\n",
    "            self.features[f\"RSI_{period}\"] = 100 - (100 / (1 + relative_strength))\n",
    "\n",
    "            #### Historical Volatility\n",
    "            self.features[f\"Hist_Vol_{period}\"] = (\n",
    "                self.dataframe[\"Returns\"].rolling(window=period).std()\n",
    "            ) * self.scaling_factor\n",
    "\n",
    "        ### VWAP-> Volume Weighted Average Price\n",
    "        self.features[\"VWAP\"] = (\n",
    "            self.dataframe[\"Close\"] * self.dataframe[\"Volume\"]\n",
    "        ).cumsum()\n",
    "\n",
    "        return self.features\n",
    "\n",
    "    ## Preparing complete dataset for the training volatility predictor model\n",
    "    def prepare_training_data(\n",
    "        self,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        Moving_AvgStd_window=22,\n",
    "        lookback_period=[5, 10, 22],\n",
    "        prediction_horizon=5,\n",
    "    ):\n",
    "        self.fetch_financial_data(start_date, end_date)\n",
    "        self.calculate_realized_volatility()\n",
    "        self.create_feature_set(Moving_AvgStd_window, lookback_period)\n",
    "\n",
    "        # Target variable -> Future Volatility\n",
    "        target = self.dataframe[\"RealizedVolatility\"].shift(-(prediction_horizon))\n",
    "\n",
    "        # Combining features and target\n",
    "        final_dataset = pd.concat(\n",
    "            [features, target], axis=1\n",
    "        ).dropna()  #### dropping the columns with missing values from the data set\n",
    "\n",
    "        # Getting the feature names before scaling\n",
    "        feature_cols = final_dataset.columns[:1]  ### all columns except the target\n",
    "\n",
    "        ### starting the standard scaler\n",
    "        scaler = ssc()\n",
    "\n",
    "        # Fit and transform the features (target not included)\n",
    "        scaled_features = scaler.fit_transform(final_dataset.iloc[:, :-1])\n",
    "\n",
    "        ## Convert scaled features back to dataframe with correct column names\n",
    "        self.scaled_features_df = pd.DataFrame(\n",
    "            scaled_features, index=final_dataset.index, columns=feature_cols\n",
    "        )\n",
    "\n",
    "        return final_dataset[\"RealizedVolatility\"], scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model to Learn Volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolatilityPredictor:\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, model_type=\"xgboost\"):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "\n",
    "    # Create XGBoost model with optimized parameters for volatility prediction\n",
    "    def create_xgboost_model(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    ):\n",
    "        return xgb.XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            min_child_weight=min_child_weight,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            objective=objective,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    # Create LSTM model for sequence-based volatility prediction\n",
    "    def create_lstm_model(\n",
    "        self,\n",
    "        input_shape,\n",
    "        no_lstm_layers=50,\n",
    "        dropout=0.2,\n",
    "        dense=1,\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\",\n",
    "    ):\n",
    "        model = seqMD(\n",
    "            [\n",
    "                LSTM(no_lstm_layers, return_sequences=True, input_shape=input_shape),\n",
    "                dpMD(dropout),\n",
    "                LSTM(no_lstm_layers),\n",
    "                dpMD(dropout),\n",
    "                Dense(dense),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        return model\n",
    "\n",
    "    # Prepare for LSTM model\n",
    "    def prepare_sequences(self, X, sequence_length=10):\n",
    "        sequences = []\n",
    "        lenX = len(X)\n",
    "        for i in range(lenX - sequence_length):\n",
    "            sequences.append(X[i : (i + sequence_length)])\n",
    "        return np.array(sequences)\n",
    "\n",
    "    # Training on Volatility prediction model\n",
    "    def train(\n",
    "        self, X, y, validation_split=0.2, target_seq_len=10, epochs=50, batch_size=32\n",
    "    ):\n",
    "        if self.model_type == \"xgboost\":\n",
    "            self.model = self.create_xgboost_model()\n",
    "            self.model.fit(X, y)\n",
    "            return\n",
    "        elif self.model_type == \"LSTM\":\n",
    "            X_seq = self.prepare_sequences(X, target_seq_len)\n",
    "            y_seq = y[target_seq_len:]\n",
    "            self.model.fit(\n",
    "                X_seq,\n",
    "                y_seq,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=validation_split,\n",
    "            )\n",
    "            return\n",
    "        return\n",
    "    \n",
    "    # Evaluation of model based on performance using multiple metrics\n",
    "    def evaluate(self,X_test,y_test):\n",
    "        if (self.model_type==\"xgboost\"):\n",
    "            prediction = self.model.predict(X_test)\n",
    "        elif (self.model_type==\"LSTM\"):\n",
    "            X_test_seq=self.prepare_sequences(X_test)\n",
    "            prediction = self.model.predict(X_test_seq)\n",
    "        \n",
    "        metrics = {\n",
    "            \"RMSE\":np.sqrt(mse(y_test,prediction)), ## root mean square error\n",
    "            \"MAE\":mae(y_test,prediction), ## mean absolute error\n",
    "            \"R2\":r2s(y_test,prediction) ## r2 score\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    # Making the volatility predictions on new data\n",
    "    def predict(self,X):\n",
    "        if (self.model_type==\"xgboost\"):\n",
    "            return self.model.predict(X)\n",
    "        elif (self.model_type==\"LSTM\"):\n",
    "            return self.model.predict(self.prepare_sequences(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mini_conda_envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
